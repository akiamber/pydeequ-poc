{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "flexible-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configparser import ConfigParser\n",
    "from mysql.connector import MySQLConnection, Error\n",
    "\n",
    "class Database_Connect:\n",
    "    \"\"\"\n",
    "    class to make DB connection\n",
    "    It accesses the config.ini file where in configurations needed for database\n",
    "    connect through mysql and pyspark are kept.\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    def __init__(self, filename='config.ini',section='mysql'):\n",
    "        self._filename=filename\n",
    "        self._section=section\n",
    "    \n",
    "    \n",
    "    def read_db_config(self):\n",
    "        parser=ConfigParser()\n",
    "        parser.read(self._filename)\n",
    "        self._db={}\n",
    "        if parser.has_section(self._section):\n",
    "            items = parser.items(self._section)\n",
    "            for item in items:\n",
    "                self._db[item[0]]=item[1]\n",
    "        else:\n",
    "            raise Exception('{0} not found in the {1} file'.format(self._section, self._filename))\n",
    "\n",
    "        return self\n",
    "    \n",
    "    def db_connect(self):\n",
    "        try:\n",
    "            print(self._db)\n",
    "            self._con = MySQLConnection(**self._db)\n",
    "            if self._con.is_connected():\n",
    "                print('connected to db')\n",
    "                return self\n",
    "            else:\n",
    "                print('not connected to db')\n",
    "                return None\n",
    "        except Error as error:\n",
    "            print('error',error)\n",
    "            \n",
    "    def close(self):\n",
    "        if self._con is not None and self._con.is_connected():\n",
    "            self._con.close()\n",
    "    \n",
    "    def get_connection(self):\n",
    "        if self._con is not None and self._con.is_connected():\n",
    "            return self._con\n",
    "        else:\n",
    "            return None\n",
    "    def get_config(self):\n",
    "        if self._db is not None:\n",
    "            return self._db\n",
    "        else:\n",
    "            return None\n",
    "        \n",
    "    def set_config(self, key, val):\n",
    "        if self._db is not None:\n",
    "            self._db[key]=val\n",
    "            return self\n",
    "        else:\n",
    "            return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "differential-chick",
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/ec2-user/spark-2.4.7-bin-hadoop2.7')\n",
    "from pyspark.sql import SparkSession, Row, DataFrame\n",
    "import pydeequ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "smoking-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "##create spark session\n",
    "spark = (SparkSession\n",
    "    .builder\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .getOrCreate())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caring-grace",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data\n",
    "file_location ='/home/ec2-user/spark-2.4.7-bin-hadoop2.7/examples/src/main/resources/1000 Records.csv'\n",
    "file_type= 'csv'\n",
    "\n",
    "#csv options\n",
    "infer_schema=\"true\"\n",
    "first_row_is_header=\"true\"\n",
    "delimiter=\",\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "tribal-enhancement",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(file_type) \\\n",
    "                .option(\"inferSchema\", infer_schema) \\\n",
    "                .option(\"header\", first_row_is_header) \\\n",
    "                .option(\"sep\", delimiter) \\\n",
    "                .load(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "failing-filling",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Emp ID: integer (nullable = true)\n",
      " |-- Name Prefix: string (nullable = true)\n",
      " |-- First Name: string (nullable = true)\n",
      " |-- Middle Initial: string (nullable = true)\n",
      " |-- Last Name: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- E Mail: string (nullable = true)\n",
      " |-- Father's Name: string (nullable = true)\n",
      " |-- Mother's Name: string (nullable = true)\n",
      " |-- Mother's Maiden Name: string (nullable = true)\n",
      " |-- Date of Birth: string (nullable = true)\n",
      " |-- Time of Birth: string (nullable = true)\n",
      " |-- Age in Yrs: double (nullable = true)\n",
      " |-- Weight in Kgs: integer (nullable = true)\n",
      " |-- Date of Joining: string (nullable = true)\n",
      " |-- Quarter of Joining: string (nullable = true)\n",
      " |-- Half of Joining: string (nullable = true)\n",
      " |-- Year of Joining: integer (nullable = true)\n",
      " |-- Month of Joining: integer (nullable = true)\n",
      " |-- Month Name of Joining: string (nullable = true)\n",
      " |-- Short Month: string (nullable = true)\n",
      " |-- Day of Joining: integer (nullable = true)\n",
      " |-- DOW of Joining: string (nullable = true)\n",
      " |-- Short DOW: string (nullable = true)\n",
      " |-- Age in Company (Years): double (nullable = true)\n",
      " |-- Salary: integer (nullable = true)\n",
      " |-- Last % Hike: string (nullable = true)\n",
      " |-- SSN: string (nullable = true)\n",
      " |-- Phone No: string (nullable = true)\n",
      " |-- Place Name: string (nullable = true)\n",
      " |-- County: string (nullable = true)\n",
      " |-- City: string (nullable = true)\n",
      " |-- State: string (nullable = true)\n",
      " |-- Zip: integer (nullable = true)\n",
      " |-- Region: string (nullable = true)\n",
      " |-- User Name: string (nullable = true)\n",
      " |-- Password: string (nullable = true)\n",
      "\n",
      "+------+-----------+----------+--------------+---------+------+--------------------+-------------+-------------+--------------------+-------------+-------------+----------+-------------+---------------+------------------+---------------+---------------+----------------+---------------------+-----------+--------------+--------------+---------+----------------------+------+-----------+-----------+------------+----------+------+---------+-----+-----+------+---------+---------------+\n",
      "|Emp ID|Name Prefix|First Name|Middle Initial|Last Name|Gender|              E Mail|Father's Name|Mother's Name|Mother's Maiden Name|Date of Birth|Time of Birth|Age in Yrs|Weight in Kgs|Date of Joining|Quarter of Joining|Half of Joining|Year of Joining|Month of Joining|Month Name of Joining|Short Month|Day of Joining|DOW of Joining|Short DOW|Age in Company (Years)|Salary|Last % Hike|        SSN|    Phone No|Place Name|County|     City|State|  Zip|Region|User Name|       Password|\n",
      "+------+-----------+----------+--------------+---------+------+--------------------+-------------+-------------+--------------------+-------------+-------------+----------+-------------+---------------+------------------+---------------+---------------+----------------+---------------------+-----------+--------------+--------------+---------+----------------------+------+-----------+-----------+------------+----------+------+---------+-----+-----+------+---------+---------------+\n",
      "|850297|        Ms.|    Shawna|             W|     Buck|     F|shawna.buck@gmail...| Rosario Buck|  Keisha Buck|           Hendricks|   12/12/1971|  06:34:47 AM|     45.66|           44|     12/18/2010|                Q4|             H2|           2010|              12|             December|        Dec|            18|      Saturday|      Sat|                  6.61|119090|        17%|222-11-7603|702-771-7149| Las Vegas| Clark|Las Vegas|   NV|89128|  West|   swbuck|ja8?k3BTF^]o@<&|\n",
      "+------+-----------+----------+--------------+---------+------+--------------------+-------------+-------------+--------------------+-------------+-------------+----------+-------------+---------------+------------------+---------------+---------------+----------------+---------------------+-----------+--------------+--------------+---------+----------------------+------+-----------+-----------+------------+----------+------+---------+-----+-----+------+---------+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()\n",
    "df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "executed-greene",
   "metadata": {},
   "outputs": [],
   "source": [
    "#store data in file\n",
    "from pydeequ.analyzers import *\n",
    "from pydeequ.repository import *\n",
    "\n",
    "metrics_file = FileSystemMetricsRepository.helper_metrics_file(spark, 'verify_newfile.json')\n",
    "repository = FileSystemMetricsRepository(spark, metrics_file)\n",
    "key_tags = {'tag': 'analyzer'}\n",
    "\n",
    "resultKey = ResultKey(spark, ResultKey.current_milli_time(), key_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "activated-study",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+--------------------+\n",
      "|     entity|            instance|                name|               value|\n",
      "+-----------+--------------------+--------------------+--------------------+\n",
      "|Mutlicolumn|Emp ID,SSN,Phone ...|       CountDistinct|              1000.0|\n",
      "|    Dataset|                   *|Size (where: Sala...|               620.0|\n",
      "|     Column|              E Mail|MaxLength (where:...|                33.0|\n",
      "|     Column|              E Mail|MinLength (where:...|                16.0|\n",
      "|     Column|              EMP ID|Distinctness (whe...|                 1.0|\n",
      "|     Column|                 SSN|Entropy (where: S...|   6.429719478039132|\n",
      "|     Column|            Phone No|PatternMatch (whe...|                 0.0|\n",
      "|Mutlicolumn|Salary,Age in Com...|Correlation (wher...|-0.02376121607231...|\n",
      "|     Column|Salary >100000 an...|          Compliance|                0.62|\n",
      "|     Column|              Salary|Histogram.bins (w...|               617.0|\n",
      "|     Column|              Salary|Histogram.abs.119...|                 1.0|\n",
      "|     Column|              Salary|Histogram.ratio.1...|               0.001|\n",
      "|     Column|              Salary|Histogram.abs.127...|                 1.0|\n",
      "|     Column|              Salary|Histogram.ratio.1...|               0.001|\n",
      "|     Column|              Salary|Histogram.abs.189...|                 1.0|\n",
      "|     Column|              Salary|Histogram.ratio.1...|               0.001|\n",
      "|     Column|              Salary|Histogram.abs.172...|                 1.0|\n",
      "|     Column|              Salary|Histogram.ratio.1...|               0.001|\n",
      "|     Column|              Salary|Histogram.abs.140...|                 1.0|\n",
      "|     Column|              Salary|Histogram.ratio.1...|               0.001|\n",
      "+-----------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "quantileProbs =[0.25,0.5,0.75,0.9]\n",
    "relError=0.0\n",
    "import pandas as pd\n",
    "#pd.set_option('display.height', 1000)\n",
    "pd.set_option('display.max_rows', 1000)\n",
    "pd.set_option('display.max_columns', 1000)\n",
    "pd.set_option('display.width', 1000)\n",
    "pd.set_option('display.max_colwidth', 1000)\n",
    "query=\"Salary >100000\"\n",
    "myresult = AnalysisRunner(spark) \\\n",
    "               .onData(df) \\\n",
    "               .addAnalyzer(Size(where=query)) \\\n",
    "               .addAnalyzer(Completeness(\"Emp ID\",query)) \\\n",
    "               .addAnalyzer(Compliance(\"Salary >100000 and 'Year of Joining' > 10\",query)) \\\n",
    "               .addAnalyzer(ApproxCountDistinct(\"Emp ID\",query)) \\\n",
    "               .addAnalyzer(ApproxQuantile(\"Salary\",0.25,0.05, query)) \\\n",
    "               .addAnalyzer(ApproxQuantiles(\"Salary\",quantileProbs,relError)) \\\n",
    "               .addAnalyzer(Correlation(\"Salary\",\"Age in Company (Years)\",query)) \\\n",
    "               .addAnalyzer(CountDistinct([\"Emp ID\",\"SSN\",\"Phone No\",\"E Mail\",\"Salary\",\"Year of Joining\"])) \\\n",
    "               .addAnalyzer(DataType(\"Emp ID\",query)) \\\n",
    "               .addAnalyzer(Distinctness(\"EMP ID\",query)) \\\n",
    "               .addAnalyzer(Distinctness([\"Age in Company (Years)\",\"Salary\"], query)) \\\n",
    "               .addAnalyzer(Entropy('SSN',query)) \\\n",
    "               .addAnalyzer(Histogram(\"Salary\",where=query)) \\\n",
    "               .addAnalyzer(Maximum('Salary',query)) \\\n",
    "               .addAnalyzer(MaxLength('E Mail',query)) \\\n",
    "               .addAnalyzer(Mean('Salary',query)) \\\n",
    "               .addAnalyzer(Minimum('Salary',query)) \\\n",
    "               .addAnalyzer(MinLength('E Mail',query)) \\\n",
    "               .addAnalyzer(MutualInformation(['Age in Yrs','Age in Company (Years)'], query)) \\\n",
    "               .addAnalyzer(StandardDeviation('Salary',query)) \\\n",
    "               .addAnalyzer(Sum('Salary',query)) \\\n",
    "               .addAnalyzer(Uniqueness(['Age in Yrs','Age in Company (Years)'],query)) \\\n",
    "               .addAnalyzer(UniqueValueRatio(['Age in Yrs','Age in Company (Years)'],query)) \\\n",
    "               .addAnalyzer(PatternMatch('Phone No',r'\\\\d{3}-\\\\d{3}-\\\\d{4}',where=query)) \\\n",
    "                .run()\n",
    "\n",
    "myresult_df = AnalyzerContext.successMetricsAsDataFrame(spark, myresult)\n",
    "myresult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "adopted-commons",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "def addUpdateRepository(tag_value: str ='my new column',df_metrics: DataFrame= None):\n",
    "               \n",
    "        df_metrics = df_metrics.withColumn('tag',F.lit(tag_value)) \\\n",
    "                               .withColumn('timestamp',F.current_timestamp().cast(StringType()))\n",
    "        df_metrics.write.format('jdbc') \\\n",
    "                  .options(**Database_Connect(section='pyspark')\n",
    "                           .read_db_config().get_config()).mode('append').save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "surprising-formula",
   "metadata": {},
   "outputs": [],
   "source": [
    "#add analysisresult dataframe to database\n",
    "addUpdateRepository('analyzer',myresult_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "handled-female",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame[months_val: string, months_val_short: string]\n"
     ]
    }
   ],
   "source": [
    "def getMonths_df():\n",
    "    \"\"\"\n",
    "    get the months from database\n",
    "    \"\"\"\n",
    "    months_df =spark.read.format('jdbc').options(**Database_Connect(section='pyspark').read_db_config().set_config('dbtable','months').get_config()).load()\n",
    "    print(months_df)\n",
    "    return months_df\n",
    "\n",
    "mylist=getMonths_df().select('months_val').rdd.map(lambda row : row[0]).collect()\n",
    "#mylist1=list(getMonths_df().select('months_val').toPandas()['months_val'])\n",
    "#print(mylist1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "imposed-reynolds",
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_func(x):\n",
    "    return x >= df.count()\n",
    "\n",
    "def completeness_func(x):\n",
    "    a=df.where((df['Salary']>100000) & (df['Age in Yrs'] >10)).count()\n",
    "    b=df.count()\n",
    "    return x ==a/b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aggregate-factor",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Callback server started!\n",
      "<pydeequ.checks.Check object at 0x7fde1b3cd390>\n"
     ]
    }
   ],
   "source": [
    "from pydeequ.checks import *\n",
    "from pydeequ.verification import *\n",
    "\n",
    "key_tags = {'tag': 'verification_Check'}\n",
    "resultKey = ResultKey(spark, ResultKey.current_milli_time(), key_tags)\n",
    "\n",
    "\n",
    "\n",
    "_check = Check(spark, CheckLevel.Error, \"Review Check\") \\\n",
    "         .hasSize(lambda x: size_func(x),'size is not greater than or equal to 1000') \\\n",
    "         .hasCompleteness(\"Emp ID\",lambda x: completeness_func(x),'Salary and Last_percent_Hike_Cleaned completeness fails for 100000 and 10 ' ) \\\n",
    "         .isComplete(\"First Name\", 'No values should be empty') \\\n",
    "         .isComplete(\"Month of Joining\") \\\n",
    "         .isContainedIn(\"Month of Joining\",mylist) \\\n",
    "         .isComplete(\"Phone No\") \\\n",
    "         .containsEmail(\"E Mail\") \\\n",
    "         .containsSocialSecurityNumber(\"SSN\") \\\n",
    "         .isUnique(\"Emp ID\")\n",
    "         \n",
    "\n",
    "checkResult = VerificationSuite(spark) \\\n",
    "    .onData(df) \\\n",
    "    .addCheck(_check) \\\n",
    "    .useRepository(repository) \\\n",
    "    .saveOrAppendResult(resultKey) \\\n",
    "    .run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "swedish-utilization",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|       check|check_level|check_status|          constraint|constraint_status|  constraint_message|\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "|Review Check|      Error|       Error|SizeConstraint(Si...|          Success|                    |\n",
      "|Review Check|      Error|       Error|CompletenessConst...|          Failure|Value: 1.0 does n...|\n",
      "|Review Check|      Error|       Error|CompletenessConst...|          Success|                    |\n",
      "|Review Check|      Error|       Error|CompletenessConst...|          Success|                    |\n",
      "|Review Check|      Error|       Error|ComplianceConstra...|          Failure|Value: 0.0 does n...|\n",
      "|Review Check|      Error|       Error|CompletenessConst...|          Success|                    |\n",
      "|Review Check|      Error|       Error|containsEmail(E M...|          Success|                    |\n",
      "|Review Check|      Error|       Error|containsSocialSec...|          Success|                    |\n",
      "|Review Check|      Error|       Error|UniquenessConstra...|          Success|                    |\n",
      "+------------+-----------+------------+--------------------+-----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkResult_df = VerificationResult.checkResultsAsDataFrame(spark, checkResult)\n",
    "checkResult_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "decent-perspective",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+--------------------+------------+------+\n",
      "| entity|            instance|        name| value|\n",
      "+-------+--------------------+------------+------+\n",
      "| Column|              Emp ID|Completeness|   1.0|\n",
      "| Column|    Month of Joining|Completeness|   1.0|\n",
      "| Column|              E Mail|PatternMatch|   1.0|\n",
      "| Column|Month of Joining ...|  Compliance|   0.0|\n",
      "|Dataset|                   *|        Size|1000.0|\n",
      "| Column|              Emp ID|  Uniqueness|   1.0|\n",
      "| Column|            Phone No|Completeness|   1.0|\n",
      "| Column|          First Name|Completeness|   1.0|\n",
      "| Column|                 SSN|PatternMatch|   1.0|\n",
      "+-------+--------------------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "checkResult_df1 = VerificationResult.successMetricsAsDataFrame(spark, checkResult)\n",
    "checkResult_df1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "extensive-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "##insert verification analysis in database\n",
    "addUpdateRepository('verification_Check',checkResult_df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worst-degree",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
